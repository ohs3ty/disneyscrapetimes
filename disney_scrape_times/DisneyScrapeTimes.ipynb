{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DisneyScrapeTimes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit (windows store)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "interpreter": {
      "hash": "6f0de41d19007a5c7438b4689e7f6de0b7426bf05e30528e1a076ec25b6f6d48"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "udqQtjJohNMC"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timezone\n",
        "import pytz"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ubwCkk-EAw"
      },
      "source": [
        "class Park:\n",
        "  park_name = ''\n",
        "  ride_arr = {};\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "class Ride:\n",
        "  ride_name = ''\n",
        "  ride_time = ''\n",
        "  def __init__(self):\n",
        "    pass\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8bFMHtgurVM"
      },
      "source": [
        "# specify which website to use in the url variable\n",
        "animal_url = \"https://queue-times.com/en-US/parks/8/queue_times\"\n",
        "epcot_url = \"https://queue-times.com/en-US/parks/5/queue_times\"\n",
        "magic_url = \"https://queue-times.com/en-US/parks/6/queue_times\"\n",
        "holly_url = \"https://queue-times.com/en-US/parks/7/queue_times\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vyCkHu3wHbx"
      },
      "source": [
        "#function to get stuff\n",
        "def web_scrape(url, park_name):\n",
        "  req = requests.get(url)\n",
        "  html = req.text\n",
        "  # request the HTML from the url specified before\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "  sections = soup.findAll(\"a\", {'class': 'panel-block'})\n",
        "\n",
        "  temp_ride_arr = []\n",
        "  for i in sections:\n",
        "    oRide = Ride()\n",
        "    oRide.ride_name = (i.find('span', {'class': \"has-text-weight-normal\"})).text\n",
        "    oRide.ride_time = (i.find('span', {'class': \"has-text-weight-bold\"})).text\n",
        "    temp_ride_arr.append(oRide)\n",
        "  oPark = Park()\n",
        "  oPark.park_name = park_name\n",
        "  oPark.ride_arr = temp_ride_arr\n",
        "  \n",
        "  return oPark\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcswmgRSS8qe"
      },
      "source": [
        "def add_to_csv(oPark):\n",
        "  from csv import writer\n",
        "  path = 'data/' + oPark.park_name.replace(' ', '_') + '.csv'\n",
        "  for ride in oPark.ride_arr:\n",
        "    list = [ride.ride_name, datetime.now(pytz.timezone('America/Detroit')), ride.ride_time]\n",
        "    with open(path, 'a') as f_object:\n",
        "      writer_object = writer(f_object)\n",
        "      writer_object.writerow(list)\n",
        "      f_object.close()\n",
        "  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEsyKGUXnUVc"
      },
      "source": [
        "def create_csv(oPark):\n",
        "  path = 'data/' + oPark.park_name.replace(' ', '_') + '.csv'\n",
        "  park_df = pd.DataFrame()\n",
        "\n",
        "  for ride in oPark.ride_arr:\n",
        "    df2 = pd.DataFrame([[ride.ride_name, datetime.now(pytz.timezone('America/Detroit')), ride.ride_time]], columns=['ride', 'datetime', 'wait_time'])\n",
        "    park_df = park_df.append(df2, ignore_index=True)\n",
        "    park_df.to_csv(path, index=False)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1mekjSwmFXK"
      },
      "source": [
        "epcot_park = web_scrape(epcot_url, 'Epcot')\n",
        "animal_park = web_scrape(animal_url, 'Animal Kingdom')\n",
        "holly_park = web_scrape(holly_url, 'Hollywood Studios')\n",
        "magic_park = web_scrape(magic_url, 'Magic Kingdom')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYV-EZaHk-wG"
      },
      "source": [
        "create_csv(animal_park)\n",
        "create_csv(epcot_park)\n",
        "create_csv(holly_park)\n",
        "create_csv(magic_park)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4KGByN8m782"
      },
      "source": [
        "add_to_csv(animal_park)\n",
        "add_to_csv(epcot_park)\n",
        "add_to_csv(holly_park)\n",
        "add_to_csv(magic_park)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}