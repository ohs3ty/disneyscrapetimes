# -*- coding: utf-8 -*-
"""DisneyScrapeTimes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YiWGnDz5xxJsNXGZeWwym9IayS3jCyGq
"""

from bs4 import BeautifulSoup
import requests
import pandas as pd
from datetime import datetime, timezone
import pytz

class Park:
  park_name = ''
  ride_arr = {}
  def __init__(self):
    pass

class Ride:
  ride_name = ''
  ride_time = ''
  def __init__(self):
    pass

# specify which website to use in the url variable
animal_url = "https://queue-times.com/en-US/parks/8/queue_times"
epcot_url = "https://queue-times.com/en-US/parks/5/queue_times"
magic_url = "https://queue-times.com/en-US/parks/6/queue_times"
holly_url = "https://queue-times.com/en-US/parks/7/queue_times"

#function to get stuff
def web_scrape(url, park_name):
  req = requests.get(url)
  html = req.text
  # request the HTML from the url specified before
  soup = BeautifulSoup(html, 'html.parser')
  sections = soup.findAll("a", {'class': 'panel-block'})

  temp_ride_arr = []
  for i in sections:
    oRide = Ride()
    oRide.ride_name = (i.find('span', {'class': "has-text-weight-normal"})).text
    oRide.ride_time = (i.find('span', {'class': "has-text-weight-bold"})).text
    temp_ride_arr.append(oRide)
  oPark = Park()
  oPark.park_name = park_name
  oPark.ride_arr = temp_ride_arr
  
  return oPark

def add_to_csv(oPark):
  from csv import writer
  path = ('data/') + oPark.park_name.replace(' ', '_') + '.csv'
  for ride in oPark.ride_arr:
    list = [ride.ride_name, datetime.now(pytz.timezone('America/Detroit')), ride.ride_time]
    with open(path, 'a') as f_object:
      writer_object = writer(f_object)
      writer_object.writerow(list)
      f_object.close()

def create_csv(oPark):
  path = ('data/') + oPark.park_name.replace(' ', '_') + '.csv'
  park_df = pd.DataFrame()

  for ride in oPark.ride_arr:
    df2 = pd.DataFrame([[ride.ride_name, datetime.now(pytz.timezone('America/Detroit')), ride.ride_time]], columns=['ride', 'datetime', 'wait_time'])
    park_df = park_df.append(df2, ignore_index=True)
    park_df.to_csv(path, index=False)

epcot_park = web_scrape(epcot_url, 'Epcot')
animal_park = web_scrape(animal_url, 'Animal Kingdom')
holly_park = web_scrape(holly_url, 'Hollywood Studios')
magic_park = web_scrape(magic_url, 'Magic Kingdom')

# create_csv(animal_park)
# create_csv(epcot_park)
# create_csv(holly_park)
# create_csv(magic_park)

add_to_csv(animal_park)
add_to_csv(epcot_park)
add_to_csv(holly_park)
add_to_csv(magic_park)